{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"PySRF","text":"<p>Discover interpretable dimensions from representational similarities.</p> <p>Representational similarity is a widely used tool in cognitive science and machine learning. PySRF decomposes similarity matrices in sparse, non-negative dimensions that reveal the latent structure underlying similarities.</p> <p>Say we have a similarity matrix \\(S\\), PySRF finds a non-negative embedding \\(W\\) such that \\(S \\approx WW^\\top\\). Each column of \\(W\\) is a dimension, and each row gives for an item a numeric weight alongside each dimensions. Because dimensions are non-negative and sparse (e.g. many entries drive toward zero), the embedding is an additive, compositional representation where dimensions contribute positively without canceling each other out.</p>","path":["PySRF"],"tags":[]},{"location":"#when-to-use-pysrf","level":2,"title":"When to use PySRF","text":"<p>PySRF works very broadly on in principle any (symmetric) similarity matix. These can come from different domains, for example:</p> <ul> <li>Behavioral data: any task that yields a measure of similarity between items.</li> <li>Neural data: representational similarity matrices derived from fMRI, electrophysiology, or other neural recordings.</li> <li>Machine learning: similarity matrices from deep neural network activations</li> </ul>","path":["PySRF"],"tags":[]},{"location":"#key-capabilities","level":2,"title":"Key capabilities","text":"<ul> <li>Missing data: real-world similarity matrices are sometimes incomplete and some entries \\(i,j\\) in a similarity matrix not observed. PySRF can handle missing entries naturally.</li> <li>Dimensionality estimation: the number of optimal dimensions can be estimated via cross-validation</li> <li>Fast solver: a Cython-accelerated solver provides 10-50x   speedup over pure Python.</li> </ul>","path":["PySRF"],"tags":[]},{"location":"#quick-example","level":2,"title":"Quick example","text":"<pre><code>import numpy as np\nfrom pysrf import SRF\n\n# Your similarity matrix (e.g., from behavioral judgments or neural data)\ns = np.random.rand(100, 100)\ns = (s + s.T) / 2\n\n# Decompose into 10 interpretable dimensions\nmodel = SRF(rank=10, max_outer=20, random_state=42)\nw = model.fit_transform(s)\n\n# Reconstruct the similarity matrix from the dimensions\ns_reconstructed = model.reconstruct()\n</code></pre>","path":["PySRF"],"tags":[]},{"location":"#reference","level":2,"title":"Reference","text":"<p>Mahner, F. P.*, Lam, K. C.*, &amp; Hebart, M. N. (2025). Interpretable dimensions from sparse representational similarities. In preparation.</p>","path":["PySRF"],"tags":[]},{"location":"#next-steps","level":2,"title":"Next steps","text":"<ul> <li>Installation: set up pysrf and compile Cython extensions.</li> <li>Quick start: walk through the core workflow step by step.</li> <li>Examples: explore advanced features and full pipelines.</li> <li>API reference: browse the complete API.</li> <li>Development: contribute to pysrf.</li> </ul>","path":["PySRF"],"tags":[]},{"location":"development/","level":1,"title":"Development","text":"","path":["Development"],"tags":[]},{"location":"development/#set-up-the-development-environment","level":2,"title":"Set up the development environment","text":"","path":["Development"],"tags":[]},{"location":"development/#automated-setup","level":3,"title":"Automated setup","text":"<p>Run the setup script to install all tools and dependencies:</p> <pre><code>chmod +x setup.sh\n./setup.sh\n</code></pre> <p>The script installs pyenv, Python 3.12.4, Poetry, all dependencies, compiles the Cython extension, and runs the test suite.</p>","path":["Development"],"tags":[]},{"location":"development/#manual-setup","level":3,"title":"Manual setup","text":"<pre><code>curl -sSL https://install.python-poetry.org | python3 -\npoetry install\nmake compile\npoetry run pytest\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#makefile-targets","level":2,"title":"Makefile targets","text":"<p>The Makefile provides shortcuts for common tasks:</p> <pre><code>make dev           # install dev dependencies and compile Cython\nmake compile       # compile Cython extension\nmake test          # run test suite\nmake test-cov      # run tests with coverage report\nmake lint          # run ruff linter\nmake format        # format code with ruff\nmake clean         # remove build artifacts\nmake docs          # build documentation with zensical\nmake docs-serve    # serve documentation locally with zensical\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#run-tests","level":2,"title":"Run tests","text":"<p>Run the full suite:</p> <pre><code>make test\n</code></pre> <p>Run tests with coverage:</p> <pre><code>make test-cov\n</code></pre> <p>Run a specific test file or test function:</p> <pre><code>poetry run pytest tests/test_model.py -v\npoetry run pytest tests/test_model.py::test_srf_fit_complete_data -v\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#code-quality","level":2,"title":"Code quality","text":"","path":["Development"],"tags":[]},{"location":"development/#lint-and-format","level":3,"title":"Lint and format","text":"<p>Check code quality and auto-format:</p> <pre><code>make lint\nmake format\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#type-hints","level":3,"title":"Type hints","text":"<p>Use Python 3.10+ style type hints in all public functions:</p> <pre><code>from __future__ import annotations\n\ndef my_function(x: np.ndarray, rank: int = 10) -&gt; tuple[np.ndarray, float]:\n    ...\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#cython-extension","level":2,"title":"Cython extension","text":"<p>The performance-critical inner loop lives in <code>_bsum.pyx</code>. Compile it with:</p> <pre><code>make compile\n\n# or\npoetry run python setup.py build_ext --inplace\n</code></pre> <p>Verify which implementation is active:</p> <pre><code>from pysrf.model import _get_update_w_function\n\nupdate_w = _get_update_w_function()\nprint(f\"Using: {update_w.__module__}\")\n# _bsum    → Cython (fast)\n# pysrf.model → pure Python fallback\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#documentation","level":2,"title":"Documentation","text":"","path":["Development"],"tags":[]},{"location":"development/#build-and-preview","level":3,"title":"Build and preview","text":"<p>pysrf uses zensical for documentation. Build and preview locally:</p> <pre><code>make docs          # build docs\nmake docs-serve    # serve locally at http://127.0.0.1:8000\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#docstring-format","level":3,"title":"Docstring format","text":"<p>Use NumPy-style docstrings:</p> <pre><code>def my_function(x: np.ndarray, param: int = 10) -&gt; float:\n    \"\"\"\n    Brief description.\n\n    Longer description explaining the function's purpose and behavior.\n\n    Parameters\n    ----------\n    x : ndarray\n        Description of x.\n    param : int, default=10\n        Description of param.\n\n    Returns\n    -------\n    result : float\n        Description of return value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = my_function(np.array([1, 2, 3]))\n    &gt;&gt;&gt; print(result)\n    0.123\n    \"\"\"\n    ...\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#contributing","level":2,"title":"Contributing","text":"<ol> <li>Fork the repository.</li> <li>Create a feature branch: <code>git checkout -b feature-name</code></li> <li>Make your changes.</li> <li>Run tests: <code>make test</code></li> <li>Format code: <code>make format</code></li> <li>Commit: <code>git commit -m \"Add feature\"</code></li> <li>Push: <code>git push origin feature-name</code></li> <li>Open a pull request.</li> </ol>","path":["Development"],"tags":[]},{"location":"development/#guidelines","level":3,"title":"Guidelines","text":"<ul> <li>Write tests for new features.</li> <li>Maintain type hints.</li> <li>Update documentation.</li> <li>Keep changes focused.</li> <li>Follow existing code style.</li> </ul>","path":["Development"],"tags":[]},{"location":"development/#publish-to-pypi","level":2,"title":"Publish to PyPI","text":"<p>Update the version in <code>pyproject.toml</code>, then build and publish:</p> <pre><code>poetry version patch  # or minor, major\nmake build\npoetry publish\n</code></pre> <p>For a pre-release version:</p> <pre><code>poetry version prerelease\npoetry publish -r testpypi\n</code></pre>","path":["Development"],"tags":[]},{"location":"development/#project-structure","level":2,"title":"Project structure","text":"<pre><code>pysrf/\n├── pysrf/              # main package\n│   ├── __init__.py     # public API\n│   ├── model.py        # SRF class\n│   ├── cross_validation.py\n│   ├── bounds.py       # sampling-bound estimation\n│   ├── utils.py        # helper functions\n│   └── _bsum.pyx       # Cython extension\n├── tests/              # test suite\n├── docs/               # documentation\n├── setup.py            # Cython build (setuptools)\n├── Makefile            # development targets\n├── pyproject.toml      # project config\n└── README.md           # project overview\n</code></pre>","path":["Development"],"tags":[]},{"location":"examples/","level":1,"title":"Examples","text":"<p>These examples show how to use pysrf for increasingly complex analyses, from basic factorization to the full pipeline with rank selection, ensemble embeddings, and evaluation.</p>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#basic-factorization","level":2,"title":"Basic factorization","text":"<p>Generate a similarity matrix from a known low-rank embedding and recover the latent dimensions:</p> <pre><code>import numpy as np\nfrom pysrf import SRF\n\n# Ground-truth low-rank structure\nn, rank = 100, 10\nw_true = np.random.rand(n, rank)\ns = w_true @ w_true.T\n\n# Recover the dimensions\nmodel = SRF(rank=10, random_state=42)\nw = model.fit_transform(s)\ns_hat = model.reconstruct()\n</code></pre> <p>Because the input has exact rank 10, the reconstruction error is near zero. In practice, real similarity matrices contain noise and the optimal rank must be estimated (see below).</p>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#missing-data","level":2,"title":"Missing data","text":"<p>Behavioral similarity experiments typically sample only a fraction of all possible item pairs. pysrf treats unobserved entries as missing rather than imputing them:</p> <pre><code>import numpy as np\nfrom pysrf import SRF\n\nn, rank = 100, 10\nw_true = np.random.rand(n, rank)\ns = w_true @ w_true.T\n\n# Remove 30% of entries to simulate sparse sampling\nmask = np.random.rand(n, n) &lt; 0.3\ns[mask] = np.nan\n\nmodel = SRF(rank=10, missing_values=np.nan, random_state=42)\nw = model.fit_transform(s)\ns_completed = model.reconstruct()\n</code></pre> <p>The model learns from the observed entries and predicts the held-out ones in the reconstruction.</p>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#cross-validation-for-rank-selection","level":2,"title":"Cross-validation for rank selection","text":"<p>The number of dimensions is a free parameter. Cross-validation selects the rank that best generalizes to unseen similarities. Set <code>estimate_sampling_fraction=True</code> to automatically derive the hold-out fraction from the data:</p> <pre><code>from pysrf import cross_val_score, SRF\n\ncv = cross_val_score(\n    s,\n    estimate_sampling_fraction=True,\n    param_grid={\"rank\": [5, 10, 15, 20]},\n    n_repeats=5,\n    n_jobs=-1,\n    random_state=42,\n)\n\nprint(f\"Best rank: {cv.best_params_['rank']}\")\nprint(f\"Best score: {cv.best_score_:.4f}\")\n</code></pre>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#consensus-embeddings","level":2,"title":"Consensus embeddings","text":"<p>Symmetric NMF is non-convex. Different random initializations can converge to different solutions. To obtain a stable embedding, fit multiple runs and align them. <code>EnsembleEmbedding</code> does this automatically: it runs the factorization <code>n_runs</code> times, aligns the columns using the Hungarian algorithm, and selects the most central solution. <code>ClusterEmbedding</code> then applies consensus clustering to find stable groups of items.</p> <pre><code>from sklearn import pipeline\nfrom pysrf.consensus import EnsembleEmbedding, ClusterEmbedding\nfrom pysrf import SRF, cross_val_score\n\n# 1. Select the rank\ncv = cross_val_score(\n    s,\n    estimate_sampling_fraction=True,\n    param_grid={\"rank\": [5, 10, 15, 20]},\n    n_repeats=5,\n    n_jobs=-1,\n)\n\n# 2. Build the ensemble-clustering pipeline\npipe = pipeline.Pipeline(\n    [\n        (\"ensemble\", EnsembleEmbedding(SRF(cv.best_params_), n_runs=50)),\n        (\"cluster\", ClusterEmbedding(min_clusters=2, max_clusters=6, step=1)),\n    ]\n)\n\nconsensus_embedding = pipe.fit_transform(s)\n</code></pre>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#value-bounds","level":2,"title":"Value bounds","text":"<p>When the similarity measure has a known range, constrain the reconstruction accordingly. For example, similarities derived from cosine distance lie in [0, 1]:</p> <pre><code>from pysrf import SRF\n\nmodel = SRF(rank=10, bounds=(0, 1), random_state=42)\nw = model.fit_transform(s)\ns_reconstructed = model.reconstruct()\n\nassert s_reconstructed.min() &gt;= 0\nassert s_reconstructed.max() &lt;= 1\n</code></pre>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#sampling-bound-estimation","level":2,"title":"Sampling-bound estimation","text":"<p>Before running cross-validation, you can estimate the range of hold-out fractions that produce reliable scores. This is useful for very sparse matrices where holding out too many entries leaves insufficient data for fitting:</p> <pre><code>from pysrf import estimate_sampling_bounds_fast\n\npmin, pmax, s_denoised = estimate_sampling_bounds_fast(\n    s,\n    n_jobs=-1,\n    random_state=42,\n)\n\nprint(f\"Minimum sampling rate: {pmin:.4f}\")\nprint(f\"Maximum sampling rate: {pmax:.4f}\")\n\n# Use the midpoint for cross-validation\nsampling_rate = 0.5 * (pmin + pmax)\n</code></pre>","path":["Get started","Examples"],"tags":[]},{"location":"examples/#complete-workflow","level":2,"title":"Complete workflow","text":"<p>A full analysis combines rank selection, ensemble fitting, and evaluation:</p> <pre><code>import numpy as np\nfrom pysrf import SRF, cross_val_score, estimate_sampling_bounds_fast\n\n# 1. Generate a noisy, incomplete similarity matrix\nnp.random.seed(42)\nn, true_rank = 100, 8\nw_true = np.random.rand(n, true_rank)\ns = w_true @ w_true.T\ns += 0.1 * np.random.randn(n, n)\ns = (s + s.T) / 2\nmask = np.random.rand(n, n) &lt; 0.2\ns[mask] = np.nan\n\n# 2. Estimate sampling bounds\npmin, pmax, _ = estimate_sampling_bounds_fast(s, n_jobs=-1)\nprint(f\"Sampling bounds: [{pmin:.3f}, {pmax:.3f}]\")\n\n# 3. Cross-validate to find the best rank\nresult = cross_val_score(\n    s,\n    param_grid={\"rank\": range(5, 21)},\n    estimate_sampling_fraction=True,\n    n_repeats=3,\n    n_jobs=-1,\n    random_state=42,\n)\nbest_rank = result.best_params_[\"rank\"]\nprint(f\"Best rank: {best_rank} (true rank: {true_rank})\")\n\n# 4. Fit the final model\nmodel = SRF(rank=best_rank, max_outer=20, random_state=42)\nw = model.fit_transform(s)\ns_completed = model.reconstruct()\n\n# 5. Evaluate\nscore = model.score(s)\nprint(f\"Reconstruction error: {score:.4f}\")\n</code></pre>","path":["Get started","Examples"],"tags":[]},{"location":"installation/","level":1,"title":"Installation","text":"<p>Cython compilation</p> <p>Compiling the Cython extension gives a 10-50x speedup. Without it, pysrf falls back to a pure Python implementation. Make sure you have a C compiler installed before proceeding.</p>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#automated-setup-recommended","level":2,"title":"Automated setup (recommended)","text":"<p>Clone the repository and run the setup script:</p> <pre><code>git clone https://github.com/fmahner/pysrf.git\ncd pysrf\n./setup.sh\n</code></pre> <p>The script performs the following steps:</p> <ol> <li>Installs <code>pyenv</code> if it is not already present.</li> <li>Installs <code>poetry</code> if it is not already present.</li> <li>Installs Python 3.12.4 through <code>pyenv</code>.</li> <li>Sets the local Python version.</li> <li>Installs all dependencies through <code>poetry</code>.</li> <li>Compiles the Cython extension.</li> <li>Runs the test suite.</li> </ol> <p>After the script finishes, activate the environment:</p> <pre><code>poetry shell\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#manual-installation","level":2,"title":"Manual installation","text":"","path":["Get started","Installation"],"tags":[]},{"location":"installation/#install-prerequisites","level":3,"title":"Install prerequisites","text":"<p>Install pyenv and poetry if you do not have them:</p> <pre><code>curl https://pyenv.run | bash\ncurl -sSL https://install.python-poetry.org | python3 -\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#set-up-python","level":3,"title":"Set up python","text":"<p>Install Python 3.12.4 (or any version &gt;=3.10) and pin it for this project:</p> <pre><code>pyenv install 3.12.4\npyenv local 3.12.4\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#install-dependencies","level":3,"title":"Install dependencies","text":"<pre><code>poetry install\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#compile-the-cython-extension","level":3,"title":"Compile the Cython extension","text":"<p>Use the Makefile target or run the build command directly:</p> <pre><code>make compile\n\n# or\npoetry run python setup.py build_ext --inplace\n</code></pre> <p>The Makefile provides additional targets:</p> <ul> <li><code>make dev</code>: install dev dependencies and compile Cython.</li> <li><code>make test</code>: run the test suite.</li> <li><code>make format</code>: format code with ruff.</li> <li><code>make clean</code>: remove build artifacts.</li> <li><code>make docs</code>: build documentation.</li> </ul>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#alternative-methods","level":2,"title":"Alternative methods","text":"","path":["Get started","Installation"],"tags":[]},{"location":"installation/#from-pypi-planned","level":3,"title":"From PyPI (planned)","text":"<pre><code>pip install pysrf\n\n# development version\npip install --pre pysrf\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#as-a-git-subtree","level":3,"title":"As a git subtree","text":"<p>Add pysrf as a subtree inside another project:</p> <pre><code>git subtree add --prefix=pysrf https://github.com/fmahner/pysrf.git master --squash\n</code></pre> <p>Update the subtree:</p> <pre><code>git subtree pull --prefix=pysrf https://github.com/fmahner/pysrf.git master --squash\n</code></pre> <p>Then install and compile:</p> <pre><code>cd pysrf &amp;&amp; poetry install &amp;&amp; make compile\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"installation/#verify-the-installation","level":2,"title":"Verify the installation","text":"<pre><code>import pysrf\nprint(pysrf.__version__)\n\n# Check whether the Cython extension is active\nfrom pysrf.model import _get_update_w_function\nupdate_w = _get_update_w_function()\nprint(f\"Using: {update_w.__module__}\")  # _bsum if compiled\n</code></pre>","path":["Get started","Installation"],"tags":[]},{"location":"quickstart/","level":1,"title":"Quick start","text":"<p>This guide walks through the core pysrf workflow: starting from a similarity matrix, recovering interpretable dimensions, handling missing data, and selecting the number of dimensions with cross-validation.</p>","path":["Get started","Quick start"],"tags":[]},{"location":"quickstart/#decompose-a-similarity-matrix","level":2,"title":"Decompose a similarity matrix","text":"<p>A similarity matrix \\(S\\) captures pairwise relationships between items. It might come from behavioral judgments, neural recordings, word associations, or model activations. pysrf factorizes \\(S\\) into a non-negative embedding \\(W\\) such that \\(S \\approx WW^\\top\\), where each column of \\(W\\) is an interpretable dimension.</p> <pre><code>import numpy as np\nfrom pysrf import SRF\n\n# Load or construct your similarity matrix\ns = np.random.rand(100, 100)\ns = (s + s.T) / 2  # ensure symmetry\n\n# Decompose into 10 dimensions\nmodel = SRF(rank=10, max_outer=20, random_state=42)\nw = model.fit_transform(s)\n\n# Reconstruct the similarity matrix from the embedding\ns_reconstructed = model.reconstruct()\n\n# Evaluate reconstruction quality\nscore = model.score(s)\nprint(f\"Reconstruction error: {score:.4f}\")\n</code></pre> <p>Each row of <code>w</code> gives an item's coordinates along the recovered dimensions. Items that load on the same dimension are similar for the same reason, making the result directly interpretable.</p>","path":["Get started","Quick start"],"tags":[]},{"location":"quickstart/#handle-missing-data","level":2,"title":"Handle missing data","text":"<p>In behavioral experiments, the number of pairwise comparisons grows quadratically with the number of items, so most similarity matrices are incomplete. pysrf handles this natively: mark missing entries as <code>NaN</code> and the model learns only from the observed pairs.</p> <pre><code>import numpy as np\nfrom pysrf import SRF\n\n# Similarity matrix with 30% of entries unobserved\ns = np.random.rand(100, 100)\ns = (s + s.T) / 2\ns[np.random.rand(100, 100) &lt; 0.3] = np.nan\n\n# Missing entries are excluded during fitting\nmodel = SRF(rank=10, missing_values=np.nan, random_state=42)\nw = model.fit_transform(s)\n\n# The reconstruction fills in the missing entries\ns_completed = model.reconstruct()\n</code></pre> <p>The completed matrix <code>s_completed</code> contains the model's predictions for both the observed and the previously missing entries.</p>","path":["Get started","Quick start"],"tags":[]},{"location":"quickstart/#select-the-number-of-dimensions","level":2,"title":"Select the number of dimensions","text":"<p>Choosing the right number of dimensions (rank) is critical. Too few dimensions miss important structure; too many overfit. pysrf provides entry-wise cross-validation: it holds out individual similarity values, fits the model on the remaining entries, and evaluates prediction accuracy on the held-out values.</p> <pre><code>from pysrf import cross_val_score\n\ns = np.random.rand(100, 100)\ns = (s + s.T) / 2\n\n# Evaluate candidate ranks\ncv = cross_val_score(s, param_grid={\"rank\": [5, 10, 15, 20]})\n\nprint(f\"Best rank: {cv.best_params_}\")\nprint(f\"Best score: {cv.best_score_:.4f}\")\n</code></pre> <p>This approach respects the dependency structure of similarity matrices, unlike standard cross-validation methods designed for independent observations.</p> <p>For consensus embeddings, sampling-bound estimation, and the full analysis pipeline, see the Examples page.</p>","path":["Get started","Quick start"],"tags":[]},{"location":"api/bounds/","level":1,"title":"Sampling Bounds API","text":"","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#main-functions","level":2,"title":"Main Functions","text":"","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#pysrf.estimate_sampling_bounds_fast","level":3,"title":"estimate_sampling_bounds_fast","text":"<pre><code>estimate_sampling_bounds_fast(\n    S: ndarray,\n    gamma: float = 1.05,\n    eta: float = 0.05,\n    rho: float = 0.95,\n    method: str = \"dyson\",\n    omega: float = 0.8,\n    eta_pmax: float = 0.001,\n    jump_frac: float = 0.1,\n    tol: float = 0.0001,\n    gap: float = 0.05,\n    random_state: int = 31213,\n    n_jobs: int = -1,\n) -&gt; tuple[float, float, np.ndarray]\n</code></pre> Source code in <code>pysrf/bounds.py</code> <pre><code>def estimate_sampling_bounds_fast(\n    S: np.ndarray,\n    gamma: float = 1.05,\n    eta: float = 0.05,\n    rho: float = 0.95,\n    method: str = \"dyson\",\n    omega: float = 0.8,\n    eta_pmax: float = 1e-3,\n    jump_frac: float = 0.1,\n    tol: float = 1e-4,\n    gap: float = 0.05,\n    random_state: int = 31213,\n    n_jobs: int = -1,\n) -&gt; tuple[float, float, np.ndarray]:\n    pmin, _, _, _, _ = pmin_bound(\n        S,\n        gamma=gamma,\n        eta=eta,\n        rho=rho,\n        random_state=random_state,\n    )\n\n    eff_dim = np.ceil((np.linalg.norm(S, \"fro\") / np.linalg.norm(S, 2)) ** 2).astype(\n        int\n    )\n\n    pmax = p_upper_only_k(\n        S,\n        k=eff_dim,\n        method=method,\n        tol=tol,\n        omega=omega,\n        eta=eta_pmax,\n        jump_frac=jump_frac,\n        seed=random_state,\n    )\n\n    S_noise = S\n\n    if pmin &gt; pmax - gap:\n        epsilon = np.linalg.norm(S, 2) / np.sqrt(S.shape[0])\n        t_range = np.linspace(0.0, epsilon, 10)\n\n        A = np.random.rand(S.shape[0], S.shape[1])\n        AtA = A + A.T\n\n        def _eval_t(t):\n            S_t = S + t * AtA\n            pmin_t, _, _, _, _ = pmin_bound(\n                S_t,\n                gamma=gamma,\n                eta=eta,\n                rho=rho,\n                random_state=random_state,\n            )\n            eff_dim_t = np.ceil(\n                (np.linalg.norm(S_t, \"fro\") / np.linalg.norm(S_t, 2)) ** 2\n            ).astype(int)\n            pmax_t = p_upper_only_k(\n                S_t,\n                k=eff_dim_t,\n                method=method,\n                tol=tol,\n                omega=omega,\n                eta=eta_pmax,\n                jump_frac=jump_frac,\n                seed=random_state,\n            )\n            return float(pmin_t), float(pmax_t)\n\n        results = Parallel(n_jobs=n_jobs)(delayed(_eval_t)(float(t)) for t in t_range)\n\n        idx = None\n        for i, (pm, px) in enumerate(results):\n            if pm &lt; px - gap:\n                idx = i\n                break\n\n        if idx is not None:\n            t_threshold = float(t_range[idx])\n            pmin, pmax = results[idx]\n        else:\n            t_threshold = 0.0\n            pmin, pmax = results[-1]\n\n        S_noise = S + t_threshold * AtA\n\n    return pmin, pmax, S_noise\n</code></pre>","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#pysrf.estimate_sampling_bounds","level":3,"title":"estimate_sampling_bounds","text":"<pre><code>estimate_sampling_bounds(\n    S: ndarray,\n    gamma: float = 1.05,\n    eta: float = 0.05,\n    rho: float = 0.95,\n    method: str = \"dyson\",\n    omega: float = 0.8,\n    eta_pmax: float = 0.001,\n    jump_frac: float = 0.1,\n    tol: float = 0.0001,\n    gap: float = 0.05,\n    random_state: int = 31213,\n) -&gt; tuple[float, float, np.ndarray]\n</code></pre> Source code in <code>pysrf/bounds.py</code> <pre><code>def estimate_sampling_bounds(\n    S: np.ndarray,\n    gamma: float = 1.05,\n    eta: float = 0.05,\n    rho: float = 0.95,\n    method: str = \"dyson\",\n    omega: float = 0.8,\n    eta_pmax: float = 1e-3,\n    jump_frac: float = 0.1,\n    tol: float = 1e-4,\n    gap: float = 0.05,\n    random_state: int = 31213,\n) -&gt; tuple[float, float, np.ndarray]:\n    pmin, _, _, _, _ = pmin_bound(\n        S,\n        gamma=gamma,\n        eta=eta,\n        rho=rho,\n        random_state=random_state,\n    )\n\n    eff_dim = np.ceil((np.linalg.norm(S, \"fro\") / np.linalg.norm(S, 2)) ** 2).astype(\n        int\n    )\n\n    pmax = p_upper_only_k(\n        S,\n        k=eff_dim,\n        method=method,\n        tol=tol,\n        omega=omega,\n        eta=eta_pmax,\n        jump_frac=jump_frac,\n        seed=random_state,\n    )\n\n    S_noise = S\n\n    if pmin &gt; pmax - gap:\n        logger.info(\"Noise regime triggered, pmin=%.4f, pmax=%.4f\", pmin, pmax)\n\n        epsilon = np.linalg.norm(S, 2) / np.sqrt(S.shape[0])\n\n        t_range = np.linspace(0.0, epsilon, 10)\n        eff_dim_list = []\n        pmin_list = []\n        pmax_list = []\n\n        A = np.random.rand(S.shape[0], S.shape[1])\n\n        t_threshold = 0\n\n        t_iter = iter(t_range)\n        t = next(t_iter)\n\n        while True:\n            S_noise = S + t * (A + A.T)\n\n            pmin, _, _, _, _ = pmin_bound(\n                S_noise,\n                gamma=gamma,\n                eta=eta,\n                rho=rho,\n                random_state=random_state,\n            )\n\n            eff_dim = np.ceil(\n                (np.linalg.norm(S_noise, \"fro\") / np.linalg.norm(S_noise, 2)) ** 2\n            ).astype(int)\n            pmax = p_upper_only_k(\n                S_noise,\n                k=eff_dim,\n                method=method,\n                tol=tol,\n                omega=omega,\n                eta=eta_pmax,\n                jump_frac=jump_frac,\n                seed=random_state,\n            )\n            logger.info(\n                \"t=%.4f, pmin=%.4f, eff_dim=%d, pmax=%.4f\", t, pmin, eff_dim, pmax\n            )\n\n            eff_dim_list.append(eff_dim)\n            pmin_list.append(pmin)\n            pmax_list.append(pmax)\n\n            if pmin &lt; pmax - gap:\n                t_threshold = t\n                break\n\n            try:\n                t = next(t_iter)\n            except StopIteration:\n                break\n\n        S_noise = S + t_threshold * (A + A.T)\n\n    return pmin, pmax, S_noise\n</code></pre>","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#lower-bound-estimation","level":2,"title":"Lower Bound Estimation","text":"","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#pysrf.pmin_bound","level":3,"title":"pmin_bound","text":"<pre><code>pmin_bound(\n    S: ndarray,\n    gamma: float = 1.05,\n    eta: float = 0.05,\n    rho: float = 0.95,\n    n_realizations: int = 500,\n    random_state: int | None = None,\n    monte_carlo: bool = False,\n) -&gt; tuple[float, float, float, float, np.ndarray]\n</code></pre> Source code in <code>pysrf/bounds.py</code> <pre><code>def pmin_bound(\n    S: np.ndarray,\n    gamma: float = 1.05,\n    eta: float = 0.05,\n    rho: float = 0.95,\n    n_realizations: int = 500,\n    random_state: int | None = None,\n    monte_carlo: bool = False,\n) -&gt; tuple[float, float, float, float, np.ndarray]:\n    np.random.seed(random_state)\n    n = S.shape[0]\n    _is_symmetric = np.allclose(S, S.T)\n\n    L_max = np.max((S**2).sum(axis=1) - np.diag(S) ** 2)\n\n    _row_sq = (S**2).sum(axis=1) - np.diag(S) ** 2\n    empirical_L_max = np.quantile(_row_sq, rho)\n\n    S_norm = np.linalg.norm(S, 2)\n\n    L_infty = 2 * np.max(np.abs(S))\n    empirical_L_infty = 2 * np.quantile(np.abs(S), rho)\n\n    effective_dimension = (np.linalg.norm(S, \"fro\") / S_norm) ** 2\n    logger.info(\"effective dimension: %s\", effective_dimension)\n\n    MC_expected_MS_norms = np.zeros(n_realizations)\n    if monte_carlo:\n        for i in range(n_realizations):\n            _p = np.random.rand()\n            _mask = np.random.binomial(1, _p, size=S.shape)\n            if _is_symmetric:\n                _mask = np.triu(_mask, 1)\n                _mask += _mask.T\n            MC_expected_MS_norms[i] = np.linalg.norm(_mask * S, 2)\n\n    N_bernstein = (gamma * L_infty * S_norm) / (3 * L_max) + 1\n    N_empirical = (gamma * empirical_L_infty * S_norm) / (3 * empirical_L_max) + 1\n    N_empirical_alternative = (gamma * L_infty * S_norm) / (3 * empirical_L_max) + 1\n    N_theory_upperbound = (gamma * S_norm) / 3 + 1\n\n    D_bernstein = ((gamma * S_norm) ** 2 / (2 * L_max) + 1) / np.log(2 * n / eta)\n    D_empirical = ((gamma * S_norm) ** 2 / (2 * empirical_L_max) + 1) / np.log(\n        2 * effective_dimension / eta\n    )\n    D_theory_lowerbound = ((gamma**2 * S_norm) / (2 * empirical_L_max) + 1) / np.log(\n        2 * n / eta\n    )\n\n    logger.info(\n        \"N_bernstein=%.4f, D_bernstein=%.4f, N_empirical=%.4f, D_empirical=%.4f, \"\n        \"N_empirical_alt=%.4f, N_theory_ub=%.4f, D_theory_lb=%.4f\",\n        N_bernstein,\n        D_bernstein,\n        N_empirical,\n        D_empirical,\n        N_empirical_alternative,\n        N_theory_upperbound,\n        D_theory_lowerbound,\n    )\n\n    p_min = N_bernstein / D_bernstein\n    p_min_empirical = N_empirical / D_empirical\n    p_min_empirical_alternative = N_empirical_alternative / D_empirical\n    p_min_lowerbound = N_theory_upperbound / D_theory_lowerbound\n\n    logger.info(\n        \"p_min=%.4f, empirical_p_min=%.4f, empirical_p_min_alt=%.4f, theory_p_min=%.4f\",\n        p_min,\n        p_min_empirical,\n        p_min_empirical_alternative,\n        p_min_lowerbound,\n    )\n\n    return (\n        p_min_empirical,\n        p_min,\n        p_min_lowerbound,\n        p_min_empirical_alternative,\n        MC_expected_MS_norms,\n    )\n</code></pre>","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#upper-bound-estimation","level":2,"title":"Upper Bound Estimation","text":"","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/bounds/#pysrf.p_upper_only_k","level":3,"title":"p_upper_only_k","text":"<pre><code>p_upper_only_k(\n    S: ndarray,\n    k: int = 1,\n    method: str = \"dyson\",\n    mc_trials: int = 600,\n    mc_quantile: float = 0.9,\n    tol: float = 0.0001,\n    seed: int | None = None,\n    omega: float = 0.8,\n    eta: float = 0.001,\n    jump_frac: float = 0.1,\n) -&gt; float\n</code></pre> Source code in <code>pysrf/bounds.py</code> <pre><code>def p_upper_only_k(\n    S: np.ndarray,\n    k: int = 1,\n    method: str = \"dyson\",\n    mc_trials: int = 600,\n    mc_quantile: float = 0.9,\n    tol: float = 1e-4,\n    seed: int | None = None,\n    omega: float = 0.8,\n    eta: float = 1e-3,\n    jump_frac: float = 0.1,\n) -&gt; float:\n    lam = np.sort(eigvalsh(S))[::-1]\n    n = len(lam)\n    if not (1 &lt;= k &lt;= n):\n        raise ValueError(\"k must be between 1 and n\")\n    lam_k = lam[k - 1]\n    lam_k1 = lam[k] if k &lt; n else None\n\n    if lam_k &lt;= 0:\n        logger.info(\"lambda_k &lt;= 0 -&gt; no positive spike to separate.\")\n        return 0.0\n    if (lam_k1 is None) or (lam_k1 &lt;= 0):\n        logger.info(\n            \"lambda_{k+1} &lt;= 0 -&gt; only first k can be out for all large p; return 1.0.\"\n        )\n        return 1.0\n\n    edge = (\n        (\n            lambda p: lambda_bulk_dyson_raw(\n                S, p, omega=omega, eta=eta, jump_frac=jump_frac\n            )\n        )\n        if method == \"dyson\"\n        else (\n            lambda p: monte_carlo_bulk_edge_raw(\n                S, p, n_trials=mc_trials, quantile=mc_quantile, seed=seed\n            )\n        )\n    )\n\n    def count_out(p):\n        e = edge(p)\n        return int(np.sum(p * lam &gt; e)), e\n\n    c_hi, e_hi = count_out(0.99)\n    logger.info(\n        \"[sanity] p=0.99: bulk=%.4g, count_out=%d, lambda1=%.4g, lambda2=%.4g\",\n        e_hi,\n        c_hi,\n        lam[0],\n        lam[1] if n &gt; 1 else np.nan,\n    )\n\n    if c_hi &lt; k:\n        logger.info(\"Even at p~1, only %d spikes out (&lt; k). Returning 1.0.\", c_hi)\n        return 1.0\n\n    grid = np.linspace(0.02, 0.99, 80)\n    feas = [p for p in grid if count_out(p)[0] == k]\n    if not feas:\n\n        def g(p):\n            return p * lam_k1 - edge(p)\n\n        a, b = 1e-3, 0.99\n        ga, gb = g(a), g(b)\n\n        if ga &gt;= 0 and gb &gt;= 0:\n            logger.info(\n                \"(k+1) spike is out for all p; returning smallest p where count==k (none found) -&gt; 0.\"\n            )\n            return 0.0\n\n        if ga &lt; 0 and gb &lt;= 0:\n            logger.info(\"(k+1) never emerges up to 0.99; returning 1.0.\")\n            return 1.0\n\n        lo, hi = a, b\n        for _ in range(60):\n            mid = 0.5 * (lo + hi)\n            if g(mid) &gt;= 0:\n                hi = mid\n            else:\n                lo = mid\n            if (hi - lo) &lt; tol:\n                break\n\n        p_star = max(0.0, min(1.0, lo - 2 * tol))\n\n        return p_star\n\n    p_lo = max(feas)\n\n    def cond_ge_kplus1(p):\n        return count_out(p)[0] &gt;= (k + 1)\n\n    p_hi = min(0.99, p_lo + 0.05)\n    while (p_hi &lt; 0.99) and (not cond_ge_kplus1(p_hi)):\n        p_hi = min(0.99, p_hi + 0.05)\n    if not cond_ge_kplus1(p_hi):\n        return 1.0\n    lo, hi = p_lo, p_hi\n    for _ in range(60):\n        mid = 0.5 * (lo + hi)\n        if cond_ge_kplus1(mid):\n            hi = mid\n        else:\n            lo = mid\n        if (hi - lo) &lt; tol:\n            break\n    p_star = max(0.0, min(1.0, lo))\n    c_star, e_star = count_out(p_star)\n    logger.info(\"p*=%.4f, bulk=%.6g, count_out(p*)=%d\", p_star, e_star, c_star)\n    return p_star\n</code></pre>","path":["API reference","Sampling Bounds API"],"tags":[]},{"location":"api/cross_validation/","level":1,"title":"Cross-Validation API","text":"","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#pysrf.cross_val_score","level":3,"title":"cross_val_score","text":"<pre><code>cross_val_score(\n    similarity_matrix: ndarray,\n    estimator: BaseEstimator | None = None,\n    param_grid: dict[str, list] | None = None,\n    n_repeats: int = 5,\n    sampling_fraction: float = 0.8,\n    estimate_sampling_fraction: bool | dict = False,\n    sampling_selection: str = \"mean\",\n    random_state: int = 0,\n    verbose: int = 1,\n    n_jobs: int = -1,\n    missing_values: float | None = np.nan,\n    fit_final_estimator: bool = False,\n) -&gt; GridSearchCV\n</code></pre> <p>Cross-validate any estimator for matrix completion.</p> <p>Generic cross-validation function that works with SRF or any sklearn-compatible estimator with a .reconstruct() method.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_matrix</code> <code>ndarray</code> <p>Symmetric similarity matrix to cross-validate</p> required <code>estimator</code> <code>BaseEstimator or None</code> <p>Estimator to cross-validate. If None, uses SRF(random_state=random_state). Can be a single estimator or a Pipeline. Must have a .reconstruct() method.</p> <code>None</code> <code>param_grid</code> <code>dict or None</code> <p>Dictionary with parameter names (str) as keys and lists of values to try as values. If None, uses default {'rank': [5, 10, 15, 20]} for SRF.</p> <code>None</code> <code>n_repeats</code> <code>int</code> <p>Number of times to repeat the cross-validation</p> <code>5</code> <code>sampling_fraction</code> <code>float</code> <p>Fraction of eligible entries to use for training in each split; must be in (0, 1). The remaining (1 - sampling_fraction) becomes validation. Note: Constant diagonal entries are excluded from both train and validation. Ignored when estimate_sampling_fraction is True or a dict; if both are provided, estimate_sampling_fraction takes precedence.</p> <code>0.8</code> <code>estimate_sampling_fraction</code> <code>bool or dict</code> <p>If True, automatically estimate optimal sampling fraction using sampling bound estimation from Random Matrix Theory. If dict, passed as kwargs to estimate_sampling_bounds_fast(). When enabled, overrides sampling_fraction.</p> <code>False</code> <code>sampling_selection</code> <code>str</code> <p>Selection method for the estimated sampling fraction; one of {\"mean\", \"min\", \"max\"}.</p> <code>\"mean\"</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility</p> <code>0</code> <code>verbose</code> <code>int</code> <p>Verbosity level</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel (-1 uses all processors)</p> <code>-1</code> <code>missing_values</code> <code>float or None</code> <p>Value to consider as missing in original data</p> <code>np.nan</code> <code>fit_final_estimator</code> <code>bool</code> <p>Whether to fit the final estimator on the best parameters</p> <code>False</code> <p>Returns:</p> Name Type Description <code>grid</code> <code>GridSearchCV</code> <p>Fitted GridSearchCV object with best parameters and scores</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pysrf.cross_validation import cross_val_score\n&gt;&gt;&gt; result = cross_val_score(similarity_matrix, param_grid={'rank': [5, 10, 15]})\n</code></pre> Source code in <code>pysrf/cross_validation.py</code> <pre><code>def cross_val_score(\n    similarity_matrix: np.ndarray,\n    estimator: BaseEstimator | None = None,\n    param_grid: dict[str, list] | None = None,\n    n_repeats: int = 5,\n    sampling_fraction: float = 0.8,\n    estimate_sampling_fraction: bool | dict = False,\n    sampling_selection: str = \"mean\",\n    random_state: int = 0,\n    verbose: int = 1,\n    n_jobs: int = -1,\n    missing_values: float | None = np.nan,\n    fit_final_estimator: bool = False,\n) -&gt; GridSearchCV:\n    \"\"\"\n    Cross-validate any estimator for matrix completion.\n\n    Generic cross-validation function that works with SRF or any sklearn-compatible\n    estimator with a .reconstruct() method.\n\n    Parameters\n    ----------\n    similarity_matrix : ndarray\n        Symmetric similarity matrix to cross-validate\n    estimator : BaseEstimator or None, default=None\n        Estimator to cross-validate. If None, uses SRF(random_state=random_state).\n        Can be a single estimator or a Pipeline. Must have a .reconstruct() method.\n    param_grid : dict or None, default=None\n        Dictionary with parameter names (str) as keys and lists of values to try\n        as values. If None, uses default {'rank': [5, 10, 15, 20]} for SRF.\n    n_repeats : int, default=5\n        Number of times to repeat the cross-validation\n    sampling_fraction : float, default=0.8\n        Fraction of eligible entries to use for training in each split; must be in (0, 1).\n        The remaining (1 - sampling_fraction) becomes validation.\n        Note: Constant diagonal entries are excluded from both train and validation.\n        Ignored when estimate_sampling_fraction is True or a dict; if both are provided,\n        estimate_sampling_fraction takes precedence.\n    estimate_sampling_fraction : bool or dict, default=False\n        If True, automatically estimate optimal sampling fraction using sampling\n        bound estimation from Random Matrix Theory. If dict, passed as kwargs to\n        estimate_sampling_bounds_fast(). When enabled, overrides sampling_fraction.\n    sampling_selection : str, default=\"mean\"\n        Selection method for the estimated sampling fraction; one of {\"mean\", \"min\", \"max\"}.\n    random_state : int, default=0\n        Random seed for reproducibility\n    verbose : int, default=1\n        Verbosity level\n    n_jobs : int, default=-1\n        Number of jobs to run in parallel (-1 uses all processors)\n    missing_values : float or None, default=np.nan\n        Value to consider as missing in original data\n    fit_final_estimator : bool, default=False\n        Whether to fit the final estimator on the best parameters\n\n    Returns\n    -------\n    grid : GridSearchCV\n        Fitted GridSearchCV object with best parameters and scores\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pysrf.cross_validation import cross_val_score\n    &gt;&gt;&gt; result = cross_val_score(similarity_matrix, param_grid={'rank': [5, 10, 15]})\n    \"\"\"\n    if estimator is None:\n        estimator = SRF(random_state=random_state)\n\n    if param_grid is None:\n        param_grid = {\"rank\": [5, 10, 15, 20]}\n\n    valid_selections = {\"mean\", \"min\", \"max\"}\n    if sampling_selection not in valid_selections:\n        raise ValueError(\n            f\"sampling_selection must be one of {sorted(valid_selections)}\"\n        )\n\n    if estimate_sampling_fraction:\n        from .bounds import estimate_sampling_bounds_fast\n\n        kwargs = (\n            estimate_sampling_fraction\n            if isinstance(estimate_sampling_fraction, dict)\n            else {}\n        )\n        if \"random_state\" not in kwargs:\n            kwargs[\"random_state\"] = random_state\n        if \"n_jobs\" not in kwargs:\n            kwargs[\"n_jobs\"] = n_jobs\n        kwargs.pop(\"verbose\", None)\n\n        pmin, pmax, s_noise = estimate_sampling_bounds_fast(similarity_matrix, **kwargs)\n        sampling_fraction = {\n            \"mean\": np.mean([pmin, pmax]),\n            \"min\": pmin,\n            \"max\": pmax,\n        }[sampling_selection]\n\n    else:\n        _validate_sampling_fraction(sampling_fraction)\n\n    cv = EntryMaskSplit(\n        n_repeats=n_repeats,\n        sampling_fraction=sampling_fraction,\n        random_state=random_state,\n        missing_values=missing_values,\n    )\n    grid = GridSearchCV(\n        estimator=estimator,\n        param_grid=param_grid,\n        cv=cv,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        fit_final_estimator=fit_final_estimator,\n    )\n    grid.fit(similarity_matrix)\n\n    return grid\n</code></pre>","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#grid-search","level":2,"title":"Grid Search","text":"","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#pysrf.GridSearchCV","level":3,"title":"GridSearchCV","text":"<pre><code>GridSearchCV(\n    estimator: BaseEstimator,\n    param_grid: dict[str, list],\n    cv: EntryMaskSplit,\n    n_jobs: int = -1,\n    verbose: int = 0,\n    fit_final_estimator: bool = False,\n)\n</code></pre> <p>Grid search cross-validation for matrix completion.</p> <p>Performs exhaustive grid search over specified parameter values with entry-wise cross-validation for symmetric matrices.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>Model instance to optimize</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameter names as keys and lists of values to try</p> required <code>cv</code> <code>EntryMaskSplit</code> <p>Cross-validation splitter</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs (-1 uses all processors)</p> <code>-1</code> <code>verbose</code> <code>int</code> <p>Verbosity level</p> <code>0</code> <code>fit_final_estimator</code> <code>bool</code> <p>Whether to fit the model on full data with best parameters</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>best_params_</code> <code>dict</code> <p>Parameters that gave the best score</p> <code>best_score_</code> <code>float</code> <p>Best validation score achieved</p> <code>cv_results_</code> <code>DataFrame</code> <p>Detailed results for all parameter combinations</p> <code>best_estimator_</code> <code>estimator</code> <p>Fitted estimator with best parameters (if fit_final_estimator=True)</p> Source code in <code>pysrf/cross_validation.py</code> <pre><code>def __init__(\n    self,\n    estimator: BaseEstimator,\n    param_grid: dict[str, list],\n    cv: EntryMaskSplit,\n    n_jobs: int = -1,\n    verbose: int = 0,\n    fit_final_estimator: bool = False,\n):\n    self.estimator = estimator\n    self.param_grid = param_grid\n    self.cv = cv\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.fit_final_estimator = fit_final_estimator\n</code></pre>","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#cv-strategy","level":2,"title":"CV Strategy","text":"","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#pysrf.EntryMaskSplit","level":3,"title":"EntryMaskSplit","text":"<pre><code>EntryMaskSplit(\n    n_repeats: int = 5,\n    sampling_fraction: float = 0.8,\n    random_state: int | None = None,\n    missing_values: float | None = np.nan,\n)\n</code></pre> <p>               Bases: <code>BaseCrossValidator</code></p> <p>Cross-validator for symmetric matrices using entry-wise splits.</p> <p>Generates multiple random train/validation splits by masking entries in a symmetric matrix while preserving symmetry.</p> <p>Parameters:</p> Name Type Description Default <code>n_repeats</code> <code>int</code> <p>Number of random splits to generate</p> <code>5</code> <code>sampling_fraction</code> <code>float</code> <p>Fraction of eligible entries kept for training; must be in (0, 1). Remaining (1 - sampling_fraction) becomes validation. Note: Constant diagonal entries are excluded from both.</p> <code>0.8</code> <code>random_state</code> <code>int or None</code> <p>Random seed for reproducibility</p> <code>None</code> <code>missing_values</code> <code>float or None</code> <p>Value that marks missing entries in original data</p> <code>np.nan</code> Source code in <code>pysrf/cross_validation.py</code> <pre><code>def __init__(\n    self,\n    n_repeats: int = 5,\n    sampling_fraction: float = 0.8,\n    random_state: int | None = None,\n    missing_values: float | None = np.nan,\n):\n    self.n_repeats = n_repeats\n    self.sampling_fraction = sampling_fraction\n    self.random_state = random_state\n    self.missing_values = missing_values\n    if not (0.0 &lt; float(self.sampling_fraction) &lt; 1.0):\n        raise ValueError(\"sampling_fraction must be in (0, 1)\")\n</code></pre>","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#pysrf.EntryMaskSplit.split","level":4,"title":"split","text":"<pre><code>split(\n    x: ndarray, y: ndarray = None, groups: ndarray = None\n) -&gt; Generator[Tuple[np.ndarray, np.ndarray], None, None]\n</code></pre> <p>Generate train/validation splits.</p> <p>Yields:</p> Name Type Description <code>train_mask</code> <code>ndarray of bool</code> <p>Training entries (True = use for training)</p> <code>validation_mask</code> <code>ndarray of bool</code> <p>Validation entries (True = use for evaluation)</p> Source code in <code>pysrf/cross_validation.py</code> <pre><code>def split(\n    self, x: np.ndarray, y: np.ndarray = None, groups: np.ndarray = None\n) -&gt; Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n    \"\"\"\n    Generate train/validation splits.\n\n    Yields\n    ------\n    train_mask : ndarray of bool\n        Training entries (True = use for training)\n    validation_mask : ndarray of bool\n        Validation entries (True = use for evaluation)\n    \"\"\"\n    rng = check_random_state(self.random_state)\n    for _ in range(self.n_repeats):\n        yield create_train_val_split(\n            x, self.sampling_fraction, rng, self.missing_values\n        )\n</code></pre>","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#scoring","level":2,"title":"Scoring","text":"","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/cross_validation/#pysrf.cross_validation.fit_and_score","level":3,"title":"fit_and_score","text":"<pre><code>fit_and_score(\n    estimator: BaseEstimator,\n    x: ndarray,\n    train_mask: ndarray,\n    validation_mask: ndarray,\n    fit_params: dict,\n    split_idx: int | None = None,\n) -&gt; dict\n</code></pre> <p>Fit estimator with parameters and return validation score.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>Model instance to fit (works with SRF or any estimator with .reconstruct())</p> required <code>x</code> <code>ndarray</code> <p>Full data matrix</p> required <code>train_mask</code> <code>ndarray of bool</code> <p>Boolean mask where True = training entry</p> required <code>validation_mask</code> <code>ndarray of bool</code> <p>Boolean mask where True = validation entry</p> required <code>fit_params</code> <code>dict</code> <p>Parameters to set on the estimator</p> required <code>split_idx</code> <code>int or None</code> <p>Index of the CV split</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>dict</code> <p>Dictionary with score, parameters, and fitted estimator</p> Source code in <code>pysrf/cross_validation.py</code> <pre><code>def fit_and_score(\n    estimator: BaseEstimator,\n    x: np.ndarray,\n    train_mask: np.ndarray,\n    validation_mask: np.ndarray,\n    fit_params: dict,\n    split_idx: int | None = None,\n) -&gt; dict:\n    \"\"\"\n    Fit estimator with parameters and return validation score.\n\n    Parameters\n    ----------\n    estimator : BaseEstimator\n        Model instance to fit (works with SRF or any estimator with .reconstruct())\n    x : ndarray\n        Full data matrix\n    train_mask : ndarray of bool\n        Boolean mask where True = training entry\n    validation_mask : ndarray of bool\n        Boolean mask where True = validation entry\n    fit_params : dict\n        Parameters to set on the estimator\n    split_idx : int or None\n        Index of the CV split\n\n    Returns\n    -------\n    result : dict\n        Dictionary with score, parameters, and fitted estimator\n    \"\"\"\n    est = clone(estimator).set_params(**fit_params)\n\n    # Set SRF-specific params if estimator supports them\n    if hasattr(est, \"missing_values\"):\n        est.set_params(missing_values=np.nan)\n\n    if hasattr(est, \"bounds\"):\n        if \"bounds\" not in fit_params or fit_params[\"bounds\"] is None:\n            original_bounds = (np.nanmin(x), np.nanmax(x))\n            est.set_params(bounds=original_bounds)\n\n    # Track which entries were already NaN in the original data\n    originally_nan = np.isnan(x)\n\n    # Create training data: keep only training entries, mask everything else\n    x_train = np.full_like(x, np.nan)\n    x_train[train_mask] = x[train_mask]\n\n    # Fit model on training data only\n    est.fit(x_train)\n\n    # Get reconstruction\n    if hasattr(est, \"reconstruct\"):\n        reconstruction = est.reconstruct()\n    else:\n        raise ValueError(\n            f\"Estimator {type(est).__name__} must have a .reconstruct() method \"\n            \"for matrix completion cross-validation\"\n        )\n\n    # Evaluate only on validation entries that were originally observed\n    valid_eval_mask = validation_mask &amp; ~originally_nan\n\n    if not valid_eval_mask.any():\n        raise ValueError(\"No valid validation entries to evaluate\")\n\n    mse = np.mean((x[valid_eval_mask] - reconstruction[valid_eval_mask]) ** 2)\n\n    result = {\n        \"score\": mse,\n        \"split\": split_idx if split_idx is not None else 0,\n        \"estimator\": est,\n        \"params\": fit_params,\n    }\n\n    # Include history if available (optional)\n    if hasattr(est, \"history_\"):\n        result[\"history\"] = est.history_\n\n    return result\n</code></pre>","path":["API reference","Cross-Validation API"],"tags":[]},{"location":"api/model/","level":1,"title":"Model API","text":"","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#srf-class","level":2,"title":"SRF Class","text":"","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#pysrf.SRF","level":3,"title":"SRF","text":"<pre><code>SRF(\n    rank: int = 10,\n    rho: float = 3.0,\n    max_outer: int = 30,\n    max_inner: int = 20,\n    tol: float = 0.0001,\n    verbose: int = 0,\n    init: str = \"random_sqrt\",\n    random_state: int | None = None,\n    missing_values: float | None = np.nan,\n    bounds: tuple[float, float] | None = (None, None),\n)\n</code></pre> <p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Symmetric Non-negative Matrix Factorization using SRF.</p> <p>This class implements symmetric non-negative matrix factorization (SymNMF) using the Alternating Direction Method of Multipliers (SRF). It can handle missing entries and optional bound constraints on the factorization.</p> <p>The algorithm solves: min_{w&gt;=0,v} ||M o (S - v)||^2_F + rho/2 ||v - ww^T||^2_F subject to optional bounds on v, where M is an observation mask.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Number of factors (dimensionality of the latent space)</p> <code>10</code> <code>rho</code> <code>float</code> <p>SRF penalty parameter controlling constraint enforcement</p> <code>3.0</code> <code>max_outer</code> <code>int</code> <p>Maximum number of SRF outer iterations</p> <code>10</code> <code>max_inner</code> <code>int</code> <p>Maximum iterations for w-subproblem per outer iteration</p> <code>30</code> <code>tol</code> <code>float</code> <p>Convergence tolerance for constraint violation</p> <code>1e-4</code> <code>verbose</code> <code>int</code> <p>Whether to print optimization progress</p> <code>0</code> <code>init</code> <code>str</code> <p>Method for factor initialization ('random', 'random_sqrt', 'nndsvd', 'nndsvdar', 'eigenspectrum')</p> <code>'random_sqrt'</code> <code>random_state</code> <code>int or None</code> <p>Random seed for reproducible initialization</p> <code>None</code> <code>missing_values</code> <code>float or None</code> <p>Values to be treated as missing to mask the matrix</p> <code>np.nan</code> <code>bounds</code> <code>tuple of (float, float) or None</code> <p>Tuple of (lower, upper) bounds for the auxiliary variable v. If None, the bounds are inferred from the data. In practice, one can also pass the expected bounds of the matrix (e.g. (0, 1) for cosine similarity)</p> <code>(None, None)</code> <p>Attributes:</p> Name Type Description <code>w_</code> <code>np.ndarray of shape (n_samples, rank)</code> <p>Learned factor matrix w</p> <code>components_</code> <code>np.ndarray of shape (n_samples, rank)</code> <p>Alias for w_ (sklearn compatibility)</p> <code>n_iter_</code> <code>int</code> <p>Number of SRF iterations performed</p> <code>history_</code> <code>dict</code> <p>Dictionary containing optimization metrics per iteration</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage with complete data\n&gt;&gt;&gt; from pysrf import SRF\n&gt;&gt;&gt; model = SRF(rank=10, random_state=42)\n&gt;&gt;&gt; w = model.fit_transform(similarity_matrix)\n&gt;&gt;&gt; reconstruction = w @ w.T\n</code></pre> <pre><code>&gt;&gt;&gt; # Usage with missing data (NaN values)\n&gt;&gt;&gt; similarity_matrix[mask] = np.nan\n&gt;&gt;&gt; model = SRF(rank=10, missing_values=np.nan)\n&gt;&gt;&gt; w = model.fit_transform(similarity_matrix)\n</code></pre> References <p>.. [1] Shi et al. (2016). \"Inexact Block Coordinate Descent Methods For        Symmetric Nonnegative Matrix Factorization\"</p> Source code in <code>pysrf/model.py</code> <pre><code>def __init__(\n    self,\n    rank: int = 10,\n    rho: float = 3.0,\n    max_outer: int = 30,\n    max_inner: int = 20,\n    tol: float = 1e-4,\n    verbose: int = 0,\n    init: str = \"random_sqrt\",\n    random_state: int | None = None,\n    missing_values: float | None = np.nan,\n    bounds: tuple[float, float] | None = (None, None),\n) -&gt; None:\n    self.rank = rank\n    self.rho = rho\n    self.max_outer = max_outer\n    self.max_inner = max_inner\n    self.tol = tol\n    self.verbose = verbose\n    self.init = init\n    self.random_state = random_state\n    self.missing_values = missing_values\n    self.bounds = bounds\n</code></pre>","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#pysrf.SRF.fit","level":4,"title":"fit","text":"<pre><code>fit(x: ndarray, y: ndarray | None = None) -&gt; SRF\n</code></pre> <p>Fit the symmetric NMF model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like of shape (n_samples, n_samples)</code> <p>Symmetric similarity matrix. Missing values are allowed and should be marked according to the missing_values parameter.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>pysrf/model.py</code> <pre><code>def fit(self, x: np.ndarray, y: np.ndarray | None = None) -&gt; SRF:\n    \"\"\"\n    Fit the symmetric NMF model to the data.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples, n_samples)\n        Symmetric similarity matrix. Missing values are allowed and should\n        be marked according to the missing_values parameter.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    self._validate_params()\n    _validate_bounds(self.bounds)\n\n    x = validate_data(\n        self,\n        x,\n        reset=True,\n        ensure_all_finite=(\n            \"allow-nan\" if _is_nan_marker(self.missing_values) else True\n        ),\n        ensure_2d=True,\n        dtype=np.float64,\n        copy=True,\n    )\n\n    self._missing_mask = _get_missing_mask(x, self.missing_values)\n    if np.all(self._missing_mask):\n        raise ValueError(\n            \"No observed entries found in the data. All values are missing.\"\n        )\n\n    check_symmetric(self._missing_mask, raise_exception=True)\n    self._observation_mask = ~self._missing_mask\n    x[self._missing_mask] = 0.0\n    x = check_symmetric(x, raise_exception=True, tol=1e-10)\n\n    if np.all(self._observation_mask):\n        return self._fit_complete_data(x)\n    else:\n        return self._fit_missing_data(x)\n</code></pre>","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#pysrf.SRF.fit_transform","level":4,"title":"fit_transform","text":"<pre><code>fit_transform(\n    x: ndarray, y: ndarray | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Fit the model and return the learned factors.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like of shape (n_samples, n_samples)</code> <p>Symmetric similarity matrix</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>w</code> <code>array-like of shape (n_samples, rank)</code> <p>Learned factor matrix</p> Source code in <code>pysrf/model.py</code> <pre><code>def fit_transform(self, x: np.ndarray, y: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Fit the model and return the learned factors.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples, n_samples)\n        Symmetric similarity matrix\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    w : array-like of shape (n_samples, rank)\n        Learned factor matrix\n    \"\"\"\n    return self.fit(x, y).transform(x)\n</code></pre>","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#pysrf.SRF.reconstruct","level":4,"title":"reconstruct","text":"<pre><code>reconstruct(w: ndarray | None = None) -&gt; np.ndarray\n</code></pre> <p>Reconstruct the similarity matrix from factors.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>array-like of shape (n_samples, rank) or None</code> <p>Factor matrix to use for reconstruction. If None, uses the fitted factors.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>s_hat</code> <code>array-like of shape (n_samples, n_samples)</code> <p>Reconstructed similarity matrix</p> Source code in <code>pysrf/model.py</code> <pre><code>def reconstruct(self, w: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Reconstruct the similarity matrix from factors.\n\n    Parameters\n    ----------\n    w : array-like of shape (n_samples, rank) or None\n        Factor matrix to use for reconstruction.\n        If None, uses the fitted factors.\n\n    Returns\n    -------\n    s_hat : array-like of shape (n_samples, n_samples)\n        Reconstructed similarity matrix\n    \"\"\"\n    if w is None:\n        check_is_fitted(self)\n        w = self.w_\n\n    return w @ w.T\n</code></pre>","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#pysrf.SRF.score","level":4,"title":"score","text":"<pre><code>score(x: ndarray, y: ndarray | None = None) -&gt; float\n</code></pre> <p>Score the model using reconstruction error on observed entries only.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like of shape (n_samples, n_samples)</code> <p>Symmetric similarity matrix. Missing values are allowed and should be marked according to the missing_values parameter.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mse</code> <code>float</code> <p>Mean squared error of the reconstruction on observed entries.</p> Source code in <code>pysrf/model.py</code> <pre><code>def score(self, x: np.ndarray, y: np.ndarray | None = None) -&gt; float:\n    \"\"\"\n    Score the model using reconstruction error on observed entries only.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples, n_samples)\n        Symmetric similarity matrix. Missing values are allowed and should\n        be marked according to the missing_values parameter.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    mse : float\n        Mean squared error of the reconstruction on observed entries.\n    \"\"\"\n    check_is_fitted(self)\n\n    x = validate_data(\n        self,\n        x,\n        reset=False,\n        ensure_2d=True,\n        dtype=np.float64,\n        ensure_all_finite=\"allow-nan\" if self.missing_values is np.nan else True,\n    )\n    observation_mask = ~_get_missing_mask(x, self.missing_values)\n    reconstruction = self.reconstruct()\n    mse = np.mean((x[observation_mask] - reconstruction[observation_mask]) ** 2)\n    return -mse\n</code></pre>","path":["API reference","Model API"],"tags":[]},{"location":"api/model/#pysrf.SRF.transform","level":4,"title":"transform","text":"<pre><code>transform(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Project data onto the learned factor space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like of shape (n_samples, n_samples)</code> <p>Symmetric matrix to transform</p> required <p>Returns:</p> Name Type Description <code>w</code> <code>array-like of shape (n_samples, rank)</code> <p>Transformed data</p> Source code in <code>pysrf/model.py</code> <pre><code>def transform(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Project data onto the learned factor space.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples, n_samples)\n        Symmetric matrix to transform\n\n    Returns\n    -------\n    w : array-like of shape (n_samples, rank)\n        Transformed data\n    \"\"\"\n    check_is_fitted(self)\n    return self.w_\n</code></pre>","path":["API reference","Model API"],"tags":[]}]}